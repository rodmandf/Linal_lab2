# Отчёт по лабораторной работе №2  
**Дисциплина:** Линейная алгебра и вычислительные модели  
**Тема:** Обучение однослойного перцептрона  

---

## 1. Цель работы
Реализовать обучение однослойного перцептрона для задачи бинарной классификации на сгенерированных данных, с отслеживанием точности и функции потерь на протяжении обучения.

---

## 2. Постановка задачи
Дана выборка из *m* объектов, каждый из которых описывается *n* признаками.  
Целевой признак \( y \in \{0, 1\} \).

**Цель:** построить модель, предсказывающую вероятность того, что \( y = 1 \), на основе признаков, и дополнительно отслеживать точность классификации на тренировочной выборке.

---

## 3. Математическая модель

### Активация и предсказание

z = X·w + b

ŷ = σ(z) = 1 / (1 + e^(-z))

**Функция потерь (кросс-энтропия):**
J(w,b) = -1/m · Σ[i=1 to m] [yⁱ·log(ŷⁱ) + (1-yⁱ)·log(1-ŷⁱ)]

**Точность:**
Accuracy = 1/m · Σ[i=1 to m] [ŷⁱ ≥ 0.6 == yⁱ]

**Обновление весов:**
w = w - α·1/m · Xᵀ(ŷ - y)
b = b - α·1/m · Σ(ŷ - y)
---

## 4. Описание реализации
- **Генерация данных.** Случайные признаки и истинные веса, бинаризация меток по признаку \( z \ge 0 \).  
- **Модель.** Класс `Perceptron` реализует веса, сигмоидальную активацию, кросс-энтропийную функцию потерь, точность и градиентный спуск.  
- **Обучение.** Ведётся подсчёт истории функции потерь и точности по эпохам. Критерий классификации: \( \hat{y} \ge 0.6 \).  
- **Отображение.** После обучения строится график изменения функции потерь и точности по эпохам.

---

## 5. Результаты
Модель обучалась на выборке из **1000 объектов**, где **800 использовались для обучения**, а **200 — для тестирования**.  
Точность модели на обучающей выборке достигла высоких значений (**> 90%**) после **200 эпох**.  

График потерь показывает убывание, а график точности — рост, что говорит о корректной реализации.

---

## 6. Выводы
В данной работе был реализован и обучен однослойный перцептрон, отслеживающий как функцию потерь, так и точность классификации.  
Реализация проведена без использования специализированных библиотек.  
Полученные графики подтверждают сходимость алгоритма и успешность градиентного обучения.
